{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Intro\n",
    "\n",
    "- Identify each step of the data wrangling process:\n",
    "\n",
    "    - gathering\n",
    "    \n",
    "    - assessing\n",
    "    \n",
    "        - quality and structure\n",
    "    \n",
    "    - cleaning\n",
    "    \n",
    "        - before analysis, visualization, or ML for predive analysis\n",
    "    \n",
    "    - we do this by using a dataset from kaggle of online job postings\n",
    "    \n",
    "- Sometimes its as simple as:\n",
    "\n",
    "    - download data, fix a few typos\n",
    "    \n",
    "    - sometimes the record isn't clean, it has missing records, duplicates, inaccurate data\n",
    "    \n",
    "    - sometimes the data is fine but structurally it is difficult to work with\n",
    "    \n",
    "    \n",
    "- IF we dont take care of these issues, we risk making mistakes, missing insites, wasting time\n",
    "\n",
    "\n",
    "- core skill we will work with everyday in data analysis process because so much of the worlds data  isnt clean\n",
    "\n",
    "\n",
    "- easy to do with code and will become instinctual\n",
    "\n",
    "    - even though we focus on Python here, we can use these concepts with any languange or application (including things like excel, sql, tableau, data engineering stack...)\n",
    "    \n",
    "- wrangling means that you round up, heard, or take charge of livestock\n",
    "\n",
    "    - think about this with sheep. A shepard needs to get their sheep graze, guide them to the market to get sheared and put them in a barn to sleep\n",
    "    \n",
    "    - before this they round them up nice in organized groups\n",
    "    \n",
    "        - if they aren't the tasks will take longer, some may get lost, and a wolf could eat some of them\n",
    "        \n",
    "    - We can think of ourselves as shepards of data. We must **organize before we act**\n",
    "    \n",
    "        - if we don't wrangle our data there could be consequences\n",
    "        \n",
    "        - If we analyze, visualize, or model our data before wrangling it we can miss out on cool insites , make mistakes, or waste time\n",
    "        \n",
    "- conclusion: best practice says to wrangle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data\n",
    "\n",
    "- Always the first step, before we gather we don't have it and afterwards we do. So this is always the first step, and also can be though of as collecting it\n",
    "\n",
    "- Steps may vary\n",
    "\n",
    "    - could download a file and import it into jupyter\n",
    "    \n",
    "    - could be files and databases (usually this is what we do in workplace)\n",
    "    \n",
    "    - We could get it from an API or scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "- https://www.kaggle.com/udacity/armenian-online-job-postings\n",
    "\n",
    "- Why it is interesting:\n",
    "\n",
    "    - the online job market is a good indicator of overall demand for labor in an economy\n",
    "    \n",
    "    - the dataset contains 19000 job postings from 2004 to 2015 posted on CareerCenter, an armenian human resource portal\n",
    "    \n",
    "    - Postings are text documents, they tend to have similar structure, text mining can be used to extract features like posting date, job title, compnay name, job descripition, slaary, and more\n",
    "    \n",
    "- How to download:\n",
    "\n",
    "    - we will download it programmatically because it is best practice\n",
    "    \n",
    "        - more scalable (we can download from 1000s of websites rather then click and point which would take forever\n",
    "        \n",
    "        - reproducability: it will allow people to reproduce your results which is important in the scientific community/ research community. It adds legitimacy to your analysis because if people can't reproduce it they won't really take it very seriously\n",
    "        \n",
    "            - dataset or website may change, so if you include the date it was downloaded they can look to access for archived copies of the dataset or understand why the results are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;35mexample-job-posting.jpg\u001b[0m  'Introduction Data Wrangling.ipynb'\r\n",
      " features.txt              online-job-postings.csv\r\n"
     ]
    }
   ],
   "source": [
    "# This is how we would have extracted it from a zip file, but we didn't have to in my environment\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('armenian-online-job-postings.zip', 'r') as myzip:\n",
    "#     myzip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather\n",
    "\n",
    "- csv files are comma seperated files that are text files\n",
    "\n",
    "- typically we get them through spreadsheet app or database export\n",
    "\n",
    "- uses comma as a delimiter\n",
    "\n",
    "- other file formats to parse are html files, .txt files, and a lot more\n",
    "\n",
    "- Pandas can import almost all file formats and database easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;35mexample-job-posting.jpg\u001b[0m  'Introduction Data Wrangling.ipynb'\r\n",
      " features.txt              online-job-postings.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"online-job-postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobpost</th>\n",
       "      <th>date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>AnnouncementCode</th>\n",
       "      <th>Term</th>\n",
       "      <th>Eligibility</th>\n",
       "      <th>Audience</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>Duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Salary</th>\n",
       "      <th>ApplicationP</th>\n",
       "      <th>OpeningDate</th>\n",
       "      <th>Deadline</th>\n",
       "      <th>Notes</th>\n",
       "      <th>AboutC</th>\n",
       "      <th>Attach</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>IT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMERIA Investment Consulting Company\\r\\nJOB TI...</td>\n",
       "      <td>Jan 5, 2004</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>AMERIA Investment Consulting Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To apply for this position, please submit a\\r\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 January 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             jobpost         date  \\\n",
       "0  AMERIA Investment Consulting Company\\r\\nJOB TI...  Jan 5, 2004   \n",
       "\n",
       "                     Title                               Company  \\\n",
       "0  Chief Financial Officer  AMERIA Investment Consulting Company   \n",
       "\n",
       "  AnnouncementCode Term Eligibility Audience StartDate Duration  ... Salary  \\\n",
       "0              NaN  NaN         NaN      NaN       NaN      NaN  ...    NaN   \n",
       "\n",
       "                                        ApplicationP OpeningDate  \\\n",
       "0  To apply for this position, please submit a\\r\\...         NaN   \n",
       "\n",
       "          Deadline Notes AboutC Attach  Year Month     IT  \n",
       "0  26 January 2004   NaN    NaN    NaN  2004     1  False  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess\n",
    "\n",
    "- we have successfully downloaded and imported our dataset\n",
    "\n",
    "- Now we need to determine what is clean and what else to gather if we are missing some data\n",
    "\n",
    "    - we are not exploring our dataset we are just making sure our data is in a form that makes it easy to analyze later on\n",
    "    \n",
    "- What area we assessing? What is dirty/  messy data\n",
    "\n",
    "    - our data's quality and tidyness\n",
    "    \n",
    "        - low quality data is dirty\n",
    "        \n",
    "        - untiy data is messy\n",
    "\n",
    "- End result: a list of observations like \"Nondescriptive column headers\"\n",
    "\n",
    "    .E.g.:\n",
    "    \n",
    "        - Data quality issues:\n",
    "        \n",
    "            - nondescriptive column headers\n",
    "            \n",
    "            - misisng vallues (NAN)\n",
    "            \n",
    "            - inconsistent representations\n",
    "            \n",
    "            - untidy data: split the data into two tables, redundant data\n",
    "            \n",
    "- Iterative approach:\n",
    "\n",
    "    - We don't want to from the beginning have to fix every issue with our data, we should only do what is necessesary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset:\n",
    "```csv\n",
    "Name, Height\n",
    "Jane, 55 inches\n",
    "Juan,\n",
    "Amalie, 145 centimetres\n",
    "Kwasi, -50 inches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Data Quality\n",
    "\n",
    "- Data quality is a perception or an assessment of data fitness to serve its purpose in a given contex\n",
    "\n",
    "- Commonly referred to as dirty data\n",
    "\n",
    "- it has issues with its content\n",
    "\n",
    "- Common data quality issues:\n",
    "    \n",
    "    - missing data \n",
    "    \n",
    "        - like for juans height\n",
    "    \n",
    "    - invlaid data \n",
    "    \n",
    "        - like negative vaulues for height (kwesi)\n",
    "        \n",
    "        - like centimeters and inches being in the height, because this makes it the wrong data type\n",
    "        \n",
    "    - inaccurate data\n",
    "    \n",
    "        - Like Jane actually being 58 inches tall, not 55\n",
    "        \n",
    "    - inconsistent data\n",
    "    \n",
    "        - Like using different units for height (inches and centimeters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untidy data\n",
    "\n",
    "- We may have to go through the normalization process to get the tidy to be in tidy format. It is very connected with normalizatioon, which would solve the process (or in general database design)\n",
    "\n",
    "- Non tidy data (linked to data denormalization) is useful sometimes. We should take a problem first approach rather then a solution backward approach.\n",
    "\n",
    "    - this means that we choose to solve a particular problem and the format to choose will most directly solve that problem or make it easier to solve rather than one that is theoretically optimal\n",
    "\n",
    "- commonly refererred to as messy data\n",
    "\n",
    "- has issues with its structure\n",
    "\n",
    "- A dataset is messy or tidy depending on how rows, columns, and tables are matched up with observations, varialbes and types\n",
    "\n",
    "    - in tidy data:\n",
    "    \n",
    "        - each variable forms a columns\n",
    "        \n",
    "        - each observation forms a row\n",
    "        \n",
    "        -  each type of observational unit forms a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to assess quality and tidyness\n",
    "\n",
    "- two styles:\n",
    "\n",
    "    - visual\n",
    "    \n",
    "        - just to look at it with eyes to assess data \n",
    "        \n",
    "        - This is easiest to do in spreadsheet application like excel\n",
    "        \n",
    "        - we can do it with pandas but it is usually easier to do with spreadsheet application\n",
    "        \n",
    "            - But sometimes if the dataset is too large it will crash if we try to use spreadsheeet application\n",
    "            \n",
    "        - Pandas is needed for large files, but scrolling through cells is not easy in pandas or fun  \n",
    "    \n",
    "    - programmatic\n",
    "    \n",
    "        - Assessing the data programmatically is using a computer program to detect problems in our data\n",
    "    \n",
    "        - Pandas is best for this. E.g. .info() function is great here\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19001 entries, 0 to 19000\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   jobpost           19001 non-null  object\n",
      " 1   date              19001 non-null  object\n",
      " 2   Title             18973 non-null  object\n",
      " 3   Company           18994 non-null  object\n",
      " 4   AnnouncementCode  1208 non-null   object\n",
      " 5   Term              7676 non-null   object\n",
      " 6   Eligibility       4930 non-null   object\n",
      " 7   Audience          640 non-null    object\n",
      " 8   StartDate         9675 non-null   object\n",
      " 9   Duration          10798 non-null  object\n",
      " 10  Location          18969 non-null  object\n",
      " 11  JobDescription    15109 non-null  object\n",
      " 12  JobRequirment     16479 non-null  object\n",
      " 13  RequiredQual      18517 non-null  object\n",
      " 14  Salary            9622 non-null   object\n",
      " 15  ApplicationP      18941 non-null  object\n",
      " 16  OpeningDate       18295 non-null  object\n",
      " 17  Deadline          18936 non-null  object\n",
      " 18  Notes             2211 non-null   object\n",
      " 19  AboutC            12470 non-null  object\n",
      " 20  Attach            1559 non-null   object\n",
      " 21  Year              19001 non-null  int64 \n",
      " 22  Month             19001 non-null  int64 \n",
      " 23  IT                19001 non-null  bool  \n",
      "dtypes: bool(1), int64(2), object(21)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean\n",
    "\n",
    "- ALWAYS make a copy of previous data before cleaning!\n",
    "\n",
    "- Most code heavy part of the data wrangling process typically\n",
    "\n",
    "- Acting on the assessments to improve quality and tidyness (we take the list we created in previous step as input in this step)\n",
    "\n",
    "- Changing the content of the data is actually data fraud, not data cleaning\n",
    "\n",
    "- we are instead correcting it when inaccurate, removing it when its wrong or irrelevant, replacing filling in missing values, or merging like when we want to combine datasets that were previously split up\n",
    "\n",
    "- Data cleaning is inefficient, error prone, and demoralizing when using text editors or spreadsheet applications\n",
    "\n",
    "    - unless it is one off issues, like changing a data type\n",
    "    \n",
    "- programmatic data cleaning is better for repetitive reproducable tasks. It can be broken down into three steps\n",
    "\n",
    "    - Define\n",
    "    \n",
    "        - We define our assessments into data cleaning tasks. This will also serve as an instruction list so others or us in the future will look at our work and reproduce it \n",
    "    \n",
    "    - Code\n",
    "    \n",
    "        - We will translate that definition into code and run that code\n",
    "    \n",
    "    - Test\n",
    "    \n",
    "        - We will run tests so we make sure they work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define. Code, Test\n",
    "\n",
    "- Define what we want to include, make instructions\n",
    "\n",
    "- This is turning list from assess phase into psuedocode for cleaning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start data inconsistency\n",
    "\n",
    "    # Find all the unique startdate ways it is listed\n",
    "    \n",
    "    # Choose one way to have them\n",
    "    \n",
    "    # Write code for that\n",
    "\n",
    "# Nondescriptive column headers\n",
    "\n",
    "     # Find what all the columns mean\n",
    "        \n",
    "    # Create a map from the current columns to renamed columns (underscores and all\n",
    "        # lowercase with desc names)\n",
    "        \n",
    "    # Replace the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code/ Test\n",
    "\n",
    "# We just code it up and write a unit test below it\n",
    "\n",
    "# Unit test would be the same thing as data validation in the ThinkStats book. We just find values that\n",
    "    # we know are correct and write a unit test for it\n",
    "    \n",
    "# Typically an `assert` statement would work well ehre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reassess and Iterate Post Cleaning\n",
    "\n",
    "- After gather, assess, and cleaning data\n",
    "\n",
    "- We always reassess after this process to see if we are happy with the quality and tidyness of data\n",
    "\n",
    "    - We can end the data wrangling process and store our data or analyze, visualize, or model our data\n",
    "    \n",
    "    - Sometimes we need to return to previous steps as we make more discoveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data (Optional)\n",
    "\n",
    "- We can store data in a filee or database if you need to use it in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Vs Exploratory Data Analysis Vs Extract Transorm Load\n",
    "\n",
    "- EDA: an analysis approach that focuses on identifying general patterns in the data, and identifying outliers and features of that data that might not have been anticipated\n",
    "\n",
    "    - We explore our data to later augment it to maximize the potential of our analyses, vislaiztion, and models. \n",
    "    \n",
    "    - We will usually use simple viz to summarzie main data characteristics\n",
    "    \n",
    "    - We can do things like remove outliers and create new and more descriptive features from data (feature engineering)\n",
    "    \n",
    "    \n",
    "- Wrangling: gathering the right pieces of data, assess data quality and tidyness, modyfing and cleaning it. It wont make the analysis, viz, or model better, just to make it possible\n",
    "\n",
    "- ETL: users are different, data is different, use cases are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afterwards\n",
    "\n",
    "- We will do  analysis and cleaning\n",
    "\n",
    "- we for example, will make a bar chart of all PMF of urgent vs not urgent start dates\n",
    "\n",
    "    - We can see that we went from 75% to 49% if we didn't wrangle the data beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
