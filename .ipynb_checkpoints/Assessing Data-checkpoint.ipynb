{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Data\n",
    "\n",
    "- the goal is immaculately clean data (so data that isn't dirty or messy)\n",
    "\n",
    "    - it is data wrangling so we are only doing this so our models, analysis, or visualizations work. Not for exploration (like feature engineering or outlier removal which make analysis better)\n",
    "    \n",
    "        - but in practice we can do both together \n",
    "\n",
    "- Like a detective at work for\n",
    "\n",
    "    - quality issues\n",
    "    \n",
    "        - content, duplicate, incorrect data\n",
    "        \n",
    "        - dirty data\n",
    "        \n",
    "    - tidiness\n",
    "    \n",
    "        - structural issues that slow you down for analysis or via or modelling\n",
    "    \n",
    "        - messy data\n",
    "        \n",
    "    - more importantly we will be able to categorize and understand the difference between messy and dirty data\n",
    "\n",
    "- We use these types of assessments\n",
    "\n",
    "    - visual\n",
    "    \n",
    "        - like excel scrolling or pandas -> df.head(5) and look at each table. But spreadsheets are better if the dataset isn't huge\n",
    "        \n",
    "        - it is best for getting aquainted with the meaning of the dataset\n",
    "        \n",
    "        - driven by the problem I want to solve, like checking the values for columns and rows I want to use in analysis\n",
    "        \n",
    "        - we can also do it undirected, by just scrolling through the dataset and seeing issues\n",
    "        \n",
    "            - then we can use more pinpointed assessments (visual or programmatic)\n",
    "      \n",
    "    - programmatic\n",
    "    \n",
    "        - through pythoon/ pandas with df.info() or using matplotlib to create visualizations to see data issues\n",
    "        \n",
    "- we then document the needed changes so we can use them in cleaning step regardless of whether we did it visually or programmatically.\n",
    "\n",
    "    - we do it by criteria of quality and tidyness\n",
    "    \n",
    "    - we don't need to write how to fix it, that is part of cleaning\n",
    "    \n",
    "        - we don't write action items here!\n",
    "        \n",
    "    - we would want to break up these steps so that we don't get overwhelmed and we can prioritize cleaning better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data we are working:\n",
    "\n",
    "    - Phase II clinical insulin test data for diabetics\n",
    "    \n",
    "    - We are comparing two drungs hba1c is property of blood, we want this metric reduced\n",
    "    \n",
    "        - .4% change is a huge thing\n",
    "        \n",
    "- healthcare data is known for dirty data\n",
    "\n",
    "    - human error during patient registration process, leads to missing, inaccurate, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty V Messy\n",
    "\n",
    "- clean = not dirty and not messy\n",
    "\n",
    "- think of a non clean bedroom:\n",
    "\n",
    "    - dirty (issues with content, inacurrate data, corrupted, data, duplicates):\n",
    "    \n",
    "        - dirty plates, garbage, dirt\n",
    "        \n",
    "        - doesn't belong in bedroom\n",
    "\n",
    "    - messy (untidy, issues with structure, broken up into columns, rows, and tables): \n",
    "    \n",
    "        - structural or organizational issues like clothes on the ground or an unmade bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of data quality issue assessment\n",
    "\n",
    "- assessment is guided by what we need to analyze, so the first thing we need to do is identify key metrics\n",
    "\n",
    "    - our key metric is `hba1c_change` because that is the measurement that can show if the drug was effective\n",
    "    \n",
    "    - we can also document less important changes as we visually assess data and identify why that might be important to clean\n",
    "    \n",
    "    - we can look at individual data point issues and document them\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "\n",
    "- treatement table: hba1c_changes: missing hbA1c changes\n",
    "\n",
    "- patients table: zip code: float instead of string data type\n",
    "\n",
    "- patients table: zip code: data type error: should be VARCHAR(5) not a float where we are missing\n",
    "\n",
    "    - this  could be important to clean if we want to automate mailing to patients\n",
    "    \n",
    "- patients table: Tim Neudorf has a height of 27 inches instead of 72, which we confirmed with checking the BMI\n",
    "\n",
    "    - important because we might want to visualize these variables\n",
    "    \n",
    "- patients table: full state names sometimes, sometimes abbreviations (data consistency)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Dimensions\n",
    "\n",
    "- Every dirty dataset is dirty in its own way, so we can't list all possible dirty data issues\n",
    "\n",
    "- meant to guide our thoughts when assessing our data\n",
    "\n",
    "- they are listed to decreasing order of severity\n",
    "\n",
    "- Dimensions (not one standard), can be called aspects, standards. These 4 are always there though\n",
    "\n",
    "    - completeness\n",
    "    \n",
    "        - do we have all of the records we should\n",
    "        \n",
    "        - do we have missing records?\n",
    "        \n",
    "        - are their specific rows, columns, or cells missing\n",
    "    \n",
    "    - validity\n",
    "    \n",
    "        - we have the records, but they aren't valid (don't conform to the defined schema)\n",
    "        \n",
    "            - schema: defined set of rules for our data\n",
    "            \n",
    "                - can be real world constraints (negative heights)\n",
    "                \n",
    "                - can be table specific constraints (unique key constraints in the table, strings for zipcodes)\n",
    "    \n",
    "    - accuracy\n",
    "    \n",
    "        - inaccurate data is wrong data that is valid\n",
    "        \n",
    "        - in other words, adheres to the correct schema but is incorrect\n",
    "        \n",
    "        - e.g. a patients weight that is 5 pounds to heavy because of poor equipment\n",
    "    \n",
    "    - consistency\n",
    "    \n",
    "        - both valid and accurate but there are multiple correct ways of reffering to the same thing\n",
    "        \n",
    "        - E.g. a standard format in columns that represent the same data across tables and/or within tables is desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Assessment\n",
    "\n",
    "- Using code to do anything other then view dataset in its entierty\n",
    "\n",
    "    - use pandas methods or matplotlib to show dirtyness or tidyness\n",
    "    \n",
    "- remember to identify key metrics first and go from there\n",
    "\n",
    "- we can also do it undirected\n",
    "\n",
    "- useful methods:\n",
    "\n",
    "    - head(), tail(), sample(), info(), describe(), .duplicated(), value_counts(), various methods of indexing/ querying data\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A lot of the times, it is good to start with .info() method\n",
    "\n",
    "    - we can see memory usage, datatypes for columns, missing rows\n",
    "    \n",
    "        - isnull() we can use to query to see missing rows\n",
    "        \n",
    "            - this will identify completness issues with missing data\n",
    "    \n",
    "    - Next it is good to check if we have appriate data types\n",
    "    \n",
    "        - usually data type issues come up when we export/ import data from different sources\n",
    "        \n",
    "        \n",
    "- Next it is good to use .describe(), we can see further data type issues here and we can see unrealisitic outliers from incorrect data entry (inaccuracy)\n",
    "\n",
    "    - note that that requires domain knowledge sometimes\n",
    "    \n",
    "- Next it is good to use .sample(), we can see random records and see inconsistent data and consistency issues\n",
    "    \n",
    "- remember to write down the quality issues to take care of in cleaning stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Dirty Data\n",
    "\n",
    "- We're going to have user entry errors.\n",
    "\n",
    "- In some situations, we won't have any data coding standards, or where we do have standards they'll be poorly applied, causing problems in the resulting data\n",
    "\n",
    "- We might have to integrate data where different schemas have been used for the same type of item.\n",
    "\n",
    "- We'll have legacy data systems, where data wasn't coded when disc and memory constraints were much more restrictive than they are now. Over time systems evolve. Needs change, and data changes.\n",
    "\n",
    "- Some of our data won't have the unique identifiers it should.\n",
    "\n",
    "- Other data will be lost in transformation from one format to another.\n",
    "\n",
    "- And then, of course, there's always programmer error.\n",
    "\n",
    "- And finally, data might have been corrupted in transmission or storage by cosmic rays or other physical phenomenon. So hey, one that's not our fault.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidyness Issues\n",
    "\n",
    "- Now we are looking with tidyness issues\n",
    "\n",
    "    - it makes analysis easier and **data cleaning easier**\n",
    "    \n",
    "- Each variable forms a columns\n",
    "\n",
    "- each observation forms a row\n",
    "\n",
    "- each type of observational unit forms a table\n",
    "\n",
    "    - this focuses on columns across all tables, so .info() is very useful here\n",
    "\n",
    "- Note: **This is directly related to normalization and denormalization which also leads to database type selections**\n",
    "\n",
    "    - 3rd normal form in normalization theory in the context of a single dataset rather then a large relational system\n",
    "\n",
    "    - we can select a database that is different the relational if normalization isn't good, or we can denormalize the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "- split phone number, email combination to two seperate columns\n",
    "\n",
    "- split up the two drug columns because they aren't a single columns and it should be split up into 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Untidy data\n",
    "\n",
    "- Messy data is usually the result of poor data planning.\n",
    "\n",
    "- Or a lack of awareness of the benefits of tidy data. \n",
    "\n",
    "- easier to address on our end then dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
